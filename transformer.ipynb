{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>AAPL US Equity</th>\n",
       "      <th>AAPL US Equity.1</th>\n",
       "      <th>AAPL US Equity.2</th>\n",
       "      <th>AAPL US Equity.3</th>\n",
       "      <th>AAPL US Equity.4</th>\n",
       "      <th>Close Tomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>field</td>\n",
       "      <td>PX_LAST</td>\n",
       "      <td>PX_OPEN</td>\n",
       "      <td>MOV_AVG_200D</td>\n",
       "      <td>MOV_AVG_5D</td>\n",
       "      <td>MOV_AVG_50D</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.9177</td>\n",
       "      <td>0.8455</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.5675</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.5706</td>\n",
       "      <td>0.9314</td>\n",
       "      <td>0.8559</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker AAPL US Equity AAPL US Equity.1 AAPL US Equity.2  \\\n",
       "0       field        PX_LAST          PX_OPEN     MOV_AVG_200D   \n",
       "1        date            NaN              NaN              NaN   \n",
       "2  2000-01-03          0.999            0.936           0.5644   \n",
       "3  2000-01-04          0.915            0.967           0.5675   \n",
       "4  2000-01-05          0.929            0.926           0.5706   \n",
       "\n",
       "  AAPL US Equity.3 AAPL US Equity.4 Close Tomorrow  \n",
       "0       MOV_AVG_5D      MOV_AVG_50D            NaN  \n",
       "1              NaN              NaN            NaN  \n",
       "2           0.9177           0.8455          0.915  \n",
       "3           0.9254           0.8506          0.929  \n",
       "4           0.9314           0.8559          0.848  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spy = pd.read_csv('newdata/AAPL.csv')\n",
    "spy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDate = spy.loc[:, 'ticker'].values\n",
    "dataLastPrice = spy.loc[:, 'AAPL US Equity'].values\n",
    "\n",
    "# pop junk out of here\n",
    "dataDate = dataDate[2:]\n",
    "dataLP = dataLastPrice[2:].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get src, trg, src_mask and trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_src_trg(\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> ([torch.tensor, torch.tensor, torch.tensor]): # TUPLE INSTEAD ***\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "        Args:\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "        Return: \n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "        #print(\"Called dataset.TransformerDataset.get_src_trg\")\n",
    "        #assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        #print(\"From data.TransformerDataset.get_src_trg: sequence shape: {}\".format(sequence.shape))\n",
    "\n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg shape before slice: {}\".format(trg.shape))\n",
    " \n",
    "        #trg = trg[:, 0]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg shape after slice: {}\".format(trg.shape))\n",
    "\n",
    "        if len(trg.shape) == 1:\n",
    "\n",
    "            trg = trg.unsqueeze(-1)\n",
    "\n",
    "            #print(\"From data.TransformerDataset.get_src_trg: trg shape after unsqueeze: {}\".format(trg.shape))\n",
    "\n",
    "\n",
    "        #assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape before slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        # We only want trg_y to consist of the target variable not any potential exogenous variables\n",
    "        #trg_y = trg_y[:, 0]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape after slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        #assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "    Source:\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    Args:\n",
    "        dim1: int, for both src and tgt masking, this must be target sequence\n",
    "              length\n",
    "        dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "    Return:\n",
    "        A Tensor of shape [dim1, dim2]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Input length\n",
    "enc_seq_len = 20\n",
    "\n",
    "# Output length\n",
    "output_sequence_length = 1\n",
    "\n",
    "# Make src mask for decoder with size:\n",
    "tgt_mask = generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length,\n",
    "    dim2=output_sequence_length\n",
    "   )\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length,\n",
    "    dim2=enc_seq_len\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The authors of the original transformer paper describe very succinctly what \n",
    "    the positional encoding layer does and why it is needed:\n",
    "    \n",
    "    \"Since our model contains no recurrence and no convolution, in order for the \n",
    "    model to make use of the order of the sequence, we must inject some \n",
    "    information about the relative or absolute position of the tokens in the \n",
    "    sequence.\" (Vaswani et al, 2017)\n",
    "    Adapted from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=5000, \n",
    "        d_model: int=512,\n",
    "        batch_first: bool=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        # copy pasted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a transformer model that can be used for times series\n",
    "    forecasting. This time series transformer model is based on the paper by\n",
    "    Wu et al (2020) [1]. The paper will be referred to as \"the paper\".\n",
    "    A detailed description of the code can be found in my article here:\n",
    "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "    In cases where the paper does not specify what value was used for a specific\n",
    "    configuration/hyperparameter, this class uses the values from Vaswani et al\n",
    "    (2017) [2] or from PyTorch source code.\n",
    "    Unlike the paper, this class assumes that input layers, positional encoding \n",
    "    layers and linear mapping layers are separate from the encoder and decoder, \n",
    "    i.e. the encoder and decoder only do what is depicted as their sub-layers \n",
    "    in the paper. For practical purposes, this assumption does not make a \n",
    "    difference - it merely means that the linear and positional encoding layers\n",
    "    are implemented inside the present class and not inside the \n",
    "    Encoder() and Decoder() classes.\n",
    "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020). \n",
    "    'Deep Transformer Models for Time Series Forecasting: \n",
    "    The Influenza Prevalence Case'. \n",
    "    arXiv:2001.08317 [cs, stat] [Preprint]. \n",
    "    Available at: http://arxiv.org/abs/2001.08317 (Accessed: 9 March 2022).\n",
    "    [2] Vaswani, A. et al. (2017) \n",
    "    'Attention Is All You Need'.\n",
    "    arXiv:1706.03762 [cs] [Preprint]. \n",
    "    Available at: http://arxiv.org/abs/1706.03762 (Accessed: 9 March 2022).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        out_seq_len: int=58,\n",
    "        dim_val: int=512,  \n",
    "        n_encoder_layers: int=4,\n",
    "        n_decoder_layers: int=4,\n",
    "        n_heads: int=8,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        num_predicted_features: int=1\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 because we're\n",
    "                                    only forecasting FCR-N prices in DK2, but in\n",
    "                                    we wanted to also predict FCR-D with the same\n",
    "                                    model, num_predicted_features should be 2.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        #print(\"input_size is: {}\".format(input_size))\n",
    "        #print(\"dim_val is: {}\".format(dim_val))\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=num_predicted_features,\n",
    "            out_features=dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=num_predicted_features\n",
    "            )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # The encoder layer used in the paper is identical to the one used by\n",
    "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerDecoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: (S,E) for unbatched input, \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input, \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        # Masking is only needed in the encoder if input sequences are padded\n",
    "        # which they are not in this time series use case, because all my\n",
    "        # input sequences are naturally of the same length. \n",
    "        # (https://github.com/huggingface/transformers/issues/4083)\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        #if src_mask is not None:\n",
    "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        #if tgt_mask is not None:\n",
    "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 20 # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 20 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 20 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 1 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    dim_val=dim_val,\n",
    "    input_size=input_size, \n",
    "    dec_seq_len=dec_seq_len,\n",
    "    batch_first=True,\n",
    "    #max_seq_len=max_seq_len,\n",
    "    out_seq_len=output_sequence_length, \n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_heads=n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # in our case, we have only 1 feature, so we need to convert `x` into [batch, sequence, features] for LSTM\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, trg, trg_y = get_src_trg(sequence=torch.FloatTensor(dataLP[idx:idx+21]), enc_seq_len=20, target_seq_len=1)\n",
    "        return (src, trg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformations():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def normalize(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        return (x-self.mu)/self.sd\n",
    "\n",
    "    def inverseNormalize(self, x):\n",
    "        return x*self.sd + self.mu\n",
    "\n",
    "transformer = Transformations()\n",
    "normLP = transformer.normalize(dataLP)\n",
    "\n",
    "def prepare_data_x(x, window_size):\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row,window_size), strides=(x.strides[0],x.strides[0]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "def prepare_data(normalized_data_close_price, plot=False):\n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_data_close_price, 20)\n",
    "    data_y = prepare_data_y(normalized_data_close_price, 20)\n",
    "\n",
    "    # split dataset\n",
    "\n",
    "    split_index_train_val = int(data_y.shape[0]*0.6)\n",
    "    split_index_val_test = int(data_y.shape[0]*0.6) + int(data_y.shape[0]*0.2)\n",
    "    data_x_train = data_x[:split_index_train_val]\n",
    "    data_x_val = data_x[split_index_train_val:split_index_val_test]\n",
    "    data_x_test = data_x[split_index_val_test:]\n",
    "\n",
    "    data_y_train = data_y[:split_index_train_val]\n",
    "    data_y_val = data_y[split_index_train_val:split_index_val_test]\n",
    "    data_y_test = data_y[split_index_val_test:]\n",
    "\n",
    "    return split_index_train_val, data_x_train, data_y_train, data_x_val, data_y_val, data_x_test, data_y_test, data_x_unseen\n",
    "\n",
    "split_index, data_x_train, data_y_train, data_x_val, data_y_val, data_x_test, data_y_test, data_x_unseen = prepare_data(dataLP)\n",
    "\n",
    "dataset_train = TransformerDataset(data_x_train, data_y_train)\n",
    "dataset_val = TransformerDataset(data_x_val, data_y_val)\n",
    "dataset_test = TransformerDataset(data_x_test, data_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 0 is 1180.5353515625\n",
      "loss in epoch 1 is 41406.45625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         output \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             src\u001b[39m=\u001b[39msrc, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             tgt\u001b[39m=\u001b[39mtrg,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             src_mask\u001b[39m=\u001b[39msrc_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             tgt_mask\u001b[39m=\u001b[39mtgt_mask\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(output, trg)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakebaldwin/Documents/binaryC-GRU-predictions/transformer.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloss in epoch\u001b[39m\u001b[39m'\u001b[39m, epoch, \u001b[39m'\u001b[39m\u001b[39mis\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem()\u001b[39m/\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "sched_step_size = 40\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=sched_step_size, gamma=0.1)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for idx, (src, trg) in enumerate(train_dataloader):\n",
    "\n",
    "        if len(src) == 20:\n",
    "            output = model(\n",
    "                src=src, \n",
    "                tgt=trg,\n",
    "                src_mask=src_mask,\n",
    "                tgt_mask=tgt_mask\n",
    "                )\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print('loss in epoch', epoch, 'is', loss.detach().item()/batch_size)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
